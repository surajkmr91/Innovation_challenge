# -*- coding: utf-8 -*-
"""Citi Finale.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z72khzFBQEIYFv7IHoIU8NI4ksRdW8jE

# Importing Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns
import datetime

"""# Uploading Dataset"""

#importing dataset --locally
from google.colab import files
uploaded = files.upload()

# importing from gdrive
# from google.colab import drive
# uploaded = files.upload()

"""## Conversion to pandas DataFrame"""

import io
df_commodity = pd.read_csv(io.BytesIO(uploaded['Dataset-Commodities.csv']))
df_currency = pd.read_csv(io.BytesIO(uploaded['Dataset-Currencies.csv']))
df_nifty50stocks = pd.read_csv(io.BytesIO(uploaded['Dataset-Nifty50Stocks.csv']))
df_stocks = pd.read_csv(io.BytesIO(uploaded['Dataset-Stocks.csv']))

"""## Data Preprocessing"""

print(df_commodity.nunique())
df_commodity.sample(5)

print(df_currency.nunique())
df_currency.head()

print(df_nifty50stocks.nunique())
df_nifty50stocks.head()
#df_nifty50stocks[df_nifty50stocks["Industry"] = ]

# myDict = {
#     'SERVICES': ['USD', 'GBP', 'SGD', 'EURO'],
#     'CONSUMER GOODS': ['USD'],
#     'FINANCIAL SERVICES' :['GOLD','CRUDEOIL','USD','SILVER'],
#     'ENERGY': ['CRUDEOIL','NATURALGAS','LEAD'],
#     'TELECOM': [''],
#     'METALS' : ['NICKEL','ZINC','ALUMINIUM','COPPER'],
#     'IT' : ['USD', 'GBP', 'SGD', 'EURO'],
#     'PHARMA' :[],
#     'CONSTRUCTION' :['USD', 'GBP', 'SGD', 'EURO','ALUMINIUM'],
#     'AUTOMOBILE' :['GOLD','CRUDEOIL','USD','SILVER','ALUMINIUM'],
#     'PHARMA' : ["CRUDEOIL",'USD', 'GBP', 'SGD'],
#     'FERTILISERS & PESTICIDES' :['ZINC'],
#     'CEMENT & CEMENT PRODUCTS' :['ALUMINIUM','LEAD','COPPER']
#           }
industry_comptability = {
    'SERVICES': [],
    'CONSUMER GOODS': [],
    'FINANCIAL SERVICES' :[],
    'ENERGY': ['CRUDEOIL','NATURALGAS'],
    'TELECOM': [''],
    'METALS' : ['NICKEL','ZINC','ALUMINIUM','COPPER', 'LEAD', 'SILVER', 'GOLD'],
    'IT' : [],
    'PHARMA' :[],
    'CONSTRUCTION' :['ALUMINIUM', 'COPPER', 'ZINC'],
    'AUTOMOBILE' :['GOLD','CRUDEOIL','SILVER','ALUMINIUM', 'LEAD', 'NICKEL', 'NATURALGAS'],
    'FERTILISERS & PESTICIDES' :['ZINC', 'LEAD'],
    'CEMENT & CEMENT PRODUCTS' :['ALUMINIUM','LEAD','COPPER', 'ZINC'],
    'MEDIA & ENTERTAINMENT' :[]
          }
industry_type = {};
for index, row in df_nifty50stocks.iterrows(): 
  industry_type[row["Symbol"]] = row["Industry"]

usd_dict = {}
gbp_dict = {}
sgd_dict = {}
euro_dict = {}
df_nifty50stocks["Currency Exposure"].fillna('', inplace=True)
for index, row in df_nifty50stocks.iterrows(): 
    com_name = row["Symbol"]
    curr_trade = row["Currency Exposure"]
    usd_dict[com_name] = 0
    gbp_dict[com_name] = 0
    sgd_dict[com_name] = 0
    euro_dict[com_name] = 0
    
    if "USD" in curr_trade :
        usd_dict[com_name] = 1
        
    if "GBP" in curr_trade :
        gbp_dict[com_name] = 1

        
    if "SGD" in curr_trade :
        sgd_dict[com_name] = 1
        
    if "EURO" in curr_trade :
        euro_dict[com_name] = 1

print(df_stocks.nunique())
df_stocks.head()

"""## Date Time Readable format"""

df_commodity["parsed_date"] = pd.to_datetime(df_commodity["DATE"], format = "%d %b %y")
df_currency["parsed_date"] = pd.to_datetime(df_currency["DATE"], format = "%d %b %y")
df_stocks["parsed_date"] = pd.to_datetime(df_stocks["DATE"], format = "%d %b %y")

print(df_commodity["parsed_date"].max(),df_commodity["parsed_date"].min())
print(df_currency["parsed_date"].max(),df_currency["parsed_date"].min())
print(df_stocks["parsed_date"].max(),df_stocks["parsed_date"].min())

df_commodity.drop(['DATE'], axis = 1,inplace = True) 
df_currency.drop(['DATE'], axis = 1,inplace = True) 
df_stocks.drop(['DATE'], axis = 1,inplace = True) 
df_currency.head()

commodity_data = df_commodity.pivot(index='parsed_date', columns='Symbol', values='CLOSE')
currency_data = df_currency.pivot(index='parsed_date', columns='SGDINR', values='CLOSE')
stocks_data = df_stocks.pivot(index='parsed_date', columns='Symbol', values='CLOSE')

"""## Merging Data

*   Stocks and Commodity
*   Stocks and Currency

Note that we are separately handling Stocks with Commodity and Stocks with Currency due to some discrepancy in dates of currency Data.
"""

df1=pd.merge(commodity_data,stocks_data, how='inner', left_index=True, right_index=True)
df2 = pd.merge(currency_data,stocks_data, how='inner', left_index=True, right_index=True)
print(commodity_data.shape)
print (currency_data.shape)
print(stocks_data.shape)
print(df1.shape)
print(df2.shape)
df2.head()

"""## Filling Missing Data
Missing data of prices filled using backward fill **(i.e. immediate past record of price)**
"""

df1.fillna(method = 'bfill', inplace = True)
df2.fillna(method = 'bfill', inplace = True)

"""# Trading Pairs Generation

## 1. Correlation Method
"""

c_name = stocks_data.columns
commdity_name = commodity_data.columns
curr_name = currency_data.columns

"""### Normalization of Data"""

from sklearn.preprocessing import StandardScaler
scaler =  StandardScaler()
new_df1 = scaler.fit_transform(df1)
scaled_df1= pd.DataFrame(new_df1, columns=df1.columns,index=df1.index)

scaler =  StandardScaler()
new_df2 = scaler.fit_transform(df2)
scaled_df2= pd.DataFrame(new_df2, columns=df2.columns,index=df2.index)

"""Yahan pe ek dataframe bnnana h"""

# dict_whole= []
# for com_name in c_name :
#     for commodity_name in commdity_name:
#         dict_temp = []
#         corr_value = df1[com_name].corr(df1[commodity_name])
#         dict_temp.append(corr_value)
#         dict_temp.append(com_name)
#         dict_temp.append(commodity_name)
#         dict_whole.append(dict_temp)


#     for currency_name in curr_name:
#         dict_temp = []
#         corr_value = df2[com_name].corr(df2[currency_name]) 
#         dict_temp.append(corr_value)
#         dict_temp.append(com_name)
#         dict_temp.append(currency_name)
#         dict_whole.append(dict_temp)

# corr_data = pd.DataFrame(dict_whole, columns = ['Corr_val', 'com_name','commodity/currency'])

"""###  Function to handle Curency Exposure of Companies"""

def currency_vs_comp(currency_name, com_name):
    # if (currency_name is "USDINR" and usd_dict[com_name]) or (currency_name is "GBPINR" and gbp_dict[com_name]) or (currency_name is "SGDINR" and sgd_dict[com_name]) or (currency_name is "EUROINR" and euro_dict[com_name]) :
    return True
    # else : 
    #     return False

# Finding Coefficient
dict_whole= []
for com_name in c_name :
    for commodity_name in commdity_name:
        dict_temp = []
        corr_value = df1[com_name].corr(df1[commodity_name])
        dict_temp.append(corr_value)
        dict_temp.append(com_name)
        dict_temp.append(commodity_name)
        dict_whole.append(dict_temp)


    for currency_name in curr_name:
        dict_temp = []
        if currency_vs_comp(currency_name,com_name):
            corr_value = df2[com_name].corr(df2[currency_name])
            dict_temp.append(corr_value)
        else :
            dict_temp.append(0);
        dict_temp.append(com_name)
        dict_temp.append(currency_name)
        dict_whole.append(dict_temp)

corr_data = pd.DataFrame(dict_whole, columns = ['Corr_val', 'com_name','commodity/currency'])

corr_data.shape

"""### Top 10 pairs based on Correlation Values"""

# TOP 10 BASED ON CORRELATION VALUE
corr_data.sort_values(by = ['Corr_val'], ascending=False).head(10)

"""### Lets plot these pairs"""

plt.figure(figsize=(25,5))
plt.subplot(1,5,1)
# pd.concat([df1["ITC"], df1["ZINC"]], axis=1).plot(figsize=(15,7))
plt.plot(scaled_df1["ONGC"])
plt.plot(scaled_df1["ZINC"])
plt.legend( ['ONGC', 'ZINC']);
plt.ylabel('Comapany_stocks vs Commodity/Currency')
plt.xlabel('TIME')
plt.xticks(rotation=90)

#plt.figure(figsize=(15,5))
plt.subplot(1,5,2)
# pd.concat([df1["ITC"], df1["ZINC"]], axis=1).plot(figsize=(15,7))
plt.plot(scaled_df1["ITC"])
plt.plot(scaled_df1["ZINC"])
plt.legend( ['ITC', 'ZINC']);
plt.ylabel('Comapany_stocks vs Commodity/Currency')
plt.xlabel('TIME')
plt.xticks(rotation=90)

#plt.figure(figsize=(15,5))
plt.subplot(1,5,3)
# pd.concat([df1["ITC"], df1["ZINC"]], axis=1).plot(figsize=(15,7))
plt.plot(scaled_df1["NESTLEIND"])
plt.plot(scaled_df1["GOLD"])
plt.legend( ['NESTLEIND', 'GOLD']);
plt.ylabel('Comapany_stocks vs Commodity/Currency')
plt.xlabel('TIME')
plt.xticks(rotation=90)

plt.subplot(1,5,4)
plt.plot(scaled_df1["HEROMOTOCO"])
plt.plot(scaled_df1["NICKEL"])
plt.legend( ['HEROMOTOCO', 'NICKEL']);
plt.ylabel('Comapany_stocks vs Commodity/Currency')
plt.xlabel('TIME')
plt.xticks(rotation=90)

plt.subplot(1,5,5)
# pd.concat([df1["ITC"], df1["ZINC"]], axis=1).plot(figsize=(15,7))
plt.plot(scaled_df1["POWERGRID"])
plt.plot(scaled_df1["COTTON"])
plt.legend( ['POWERGRID', 'COTTON']);
plt.ylabel('Comapany_stocks vs Commodity/Currency')
plt.xlabel('TIME')
plt.xticks(rotation=90)

"""From the plot of NESTLE AND GOLD (3rd plot) we can see that there is no mean returning value and hence in this case no trade will be possible by trading pair"""

print (corr_data[corr_data["Corr_val"]>.75].shape)
print (corr_data[corr_data["Corr_val"]>.6].shape)
new_data = corr_data[corr_data["Corr_val"]>.6]

"""## 2. Method of Co-Integration Test (Convergence information)
####  computing the p-value of the cointegration test will inform us as to whether the ratio between the two timeseries is stationary around its mean
"""

#importing module for co-integration value
from statsmodels.tsa.stattools import coint

dict_whole= []
for com_name in c_name :
    for commodity_name in commdity_name:
        # print(com_name)
        # print(commodity_name)
        dict_temp = []
        corr_value = df1[com_name].corr(df1[commodity_name])
        _, pvalue, _ = coint(df1[com_name], df1[commodity_name]) # p value from co-integration test
        # print(pvalue)
        dict_temp.append(corr_value)
        dict_temp.append(pvalue)
        dict_temp.append(com_name)
        dict_temp.append(commodity_name)
        dict_whole.append(dict_temp)


    for currency_name in curr_name:
        dict_temp = []
        if currency_vs_comp(currency_name,com_name):
            corr_value = df2[com_name].corr(df2[currency_name])
            _, pvalue, _ = coint(df2[com_name], df2[currency_name]) # p value from co-integration test
            dict_temp.append(corr_value)
            dict_temp.append(pvalue)
        else :
            dict_temp.append(0)
            dict_temp.append(1)
        dict_temp.append(com_name)
        dict_temp.append(currency_name)
        dict_whole.append(dict_temp)

coint_data = pd.DataFrame(dict_whole, columns = ['Corr_val', 'pvalue', 'com_name','commodity/currency'])

"""###  TOP 10 trading pairs based on Cointegration Test result"""

# TOP 10 BASED ON COINTEGRATION TEST ONLY
coint_data.sort_values(by = ['pvalue'], ascending=True).head(10)

"""### Lets plot these pairs"""

plt.figure(figsize=(25,5))
plt.subplot(1,5,1)
plt.plot(scaled_df1["ONGC"])
plt.plot(scaled_df1["ZINC"])
plt.legend( ['ONGC', 'ZINC']);
plt.ylabel('Comapany_stocks vs Commodity/Currency')
plt.xlabel('TIME')
plt.xticks(rotation=90)

plt.subplot(1,5,2)
plt.plot(scaled_df1["INDUSINDBK"])
plt.plot(scaled_df1["CRUDEOIL"])
plt.legend( ['INDUSINDBK', 'CRUDEOIL']);
plt.ylabel('Comapany_stocks vs Commodity/Currency')
plt.xlabel('TIME')
plt.xticks(rotation=90)

plt.subplot(1,5,3)
plt.plot(scaled_df1["ITC"])
plt.plot(scaled_df1["ZINC"])
plt.legend( ['ITC', 'ZINC']);
plt.ylabel('Comapany_stocks vs Commodity/Currency')
plt.xlabel('TIME')
plt.xticks(rotation=90)

plt.subplot(1,5,4)
plt.plot(scaled_df1["COALINDIA"])
plt.plot(scaled_df1["ZINC"])
plt.legend( ['COALINDIA', 'ZINC']);
plt.ylabel('Comapany_stocks vs Commodity/Currency')
plt.xlabel('TIME')
plt.xticks(rotation=90)

plt.subplot(1,5,5)
plt.plot(scaled_df1["UPL"])
plt.plot(scaled_df1["NATURALGAS"])
plt.legend( ['UPL', 'NATURALGAS']);
plt.ylabel('Comapany_stocks vs Commodity/Currency')
plt.xlabel('TIME')
plt.xticks(rotation=90)

"""### From the 5th plot (UPL and NATURAL GAS) we can see altough it has good p value but it is less correlated with each other. Hence we should consider both while ranking Trading Pairs

## 3. The distance method
"""

dict_whole= []
for com_name in c_name :
    for commodity_name in commdity_name:
        dict_temp = []
        dict_temp.append(com_name)
        dict_temp.append(commodity_name)
        norm_a = np.array(scaled_df1[com_name])
        norm_b = np.array(scaled_df1[commodity_name])
        dist = sum((norm_a - norm_b)**2) # Euclidean Distance
        dict_temp.append(dist)
        dict_whole.append(dict_temp)

    for currency_name in curr_name:
        dict_temp = []
        dict_temp.append(com_name)
        dict_temp.append(currency_name)
        if currency_vs_comp(currency_name,com_name):
            norm_a = np.array(scaled_df2[com_name])
            norm_b = np.array(scaled_df2[currency_name]) 
            dist = sum((norm_a - norm_b)**2) #Euclidean Distance
            dict_temp.append(dist)
        else :
            dict_temp.append(1e6)
        
        dict_whole.append(dict_temp)
dist_data = pd.DataFrame(dict_whole, columns = ['com_name','commodity/currency',"dis_val"])

"""## TOP 10 Trading pairs based on the Distance Method
Lower the distance more is the pair suitable
"""

dist_data.sort_values(by = ['dis_val'], ascending=True).head(10)

"""### Lets plot these pairs"""

plt.figure(figsize=(25,5))
plt.subplot(1,5,1)
plt.plot(scaled_df1["ONGC"])
plt.plot(scaled_df1["ZINC"])
plt.legend( ['ONGC', 'ZINC']);
plt.ylabel('Comapany_stocks vs Commodity/Currency')
plt.xlabel('TIME')
plt.xticks(rotation=90)

#plt.figure(figsize=(15,5))
plt.subplot(1,5,2)
plt.plot(scaled_df1["ITC"])
plt.plot(scaled_df1["ZINC"])
plt.legend( ['ITC', 'ZINC']);
plt.ylabel('Comapany_stocks vs Commodity/Currency')
plt.xlabel('TIME')
plt.xticks(rotation=90)

plt.subplot(1,5,3)
plt.plot(scaled_df1["NESTLEIND"])
plt.plot(scaled_df1["GOLD"])
plt.legend( ['NESTLEIND', 'GOLD']);
plt.ylabel('Comapany_stocks vs Commodity/Currency')
plt.xlabel('TIME')
plt.xticks(rotation=90)

plt.subplot(1,5,4)
plt.plot(scaled_df1["HEROMOTOCO"])
plt.plot(scaled_df1["NICKEL"])
plt.legend( ['HEROMOTOCO', 'NICKEL']);
plt.ylabel('Comapany_stocks vs Commodity/Currency')
plt.xlabel('TIME')
plt.xticks(rotation=90)

plt.subplot(1,5,5)
plt.plot(scaled_df1["POWERGRID"])
plt.plot(scaled_df1["COTTON"])
plt.legend( ['POWERGRID', 'COTTON']);
plt.ylabel('Comapany_stocks vs Commodity/Currency')
plt.xlabel('TIME')
plt.xticks(rotation=90)

# NO NEED, MAYBE DELETE IT TODAY. DISCUSS FIRST
processed_data = coint_data.copy()
processed_data["dist_val"] = dist_data["dis_val"]  
processed_data = processed_data[(processed_data["Corr_val"]> 0) & (processed_data["pvalue"]<1)& (processed_data["dist_val"]<1000) ]
processed_data.shape

"""## 4. Method of Mean Reversion
Using **Hurst Exponent** in pair trading
"""

#Module for hurst exponent calculation
!pip install hurst
from hurst import compute_Hc
from hurst import random_walk

from sklearn.linear_model import LinearRegression

dict_whole= []
for com_name in c_name :
    for commodity_name in commdity_name:
        dict_temp = []
	# Using HURST EXPONENT TECHNIQUE
    #model for y (commodity_name) = x (company stocks) * n+ residue
    # n = number of stocks to buy for one commodity or currency
    # NEW SERIES: SPREAD = log(x) - n * log(y)
    # n calculated from regression coefficient
        

        model = LinearRegression().fit(df1[com_name][:, None], df1[commodity_name])
        n = model.coef_
        newSeries = pd.DataFrame()
        newSeries['spread'] = np.log(df1[com_name]) - n * np.log10(df1[commodity_name])
        H = 0.0
        try :
            H, _, _ = compute_Hc(newSeries, kind='price', simplified=True)
        except : 
            H = 0.5
        # print("h : ", H)

        dict_temp.append(H)
        dict_temp.append(com_name)
        dict_temp.append(commodity_name)
        dict_whole.append(dict_temp)


    for currency_name in curr_name:

        model = LinearRegression().fit(df2[com_name][:, None], df2[currency_name])
        n = model.coef_
        newSeries = pd.DataFrame()
        newSeries['spread'] = np.log(df2[com_name]) - n * np.log10(df2[currency_name])
        H = 0.0
        try :
            H, _, _ = compute_Hc(newSeries, kind='price', simplified=True)
        except : 
            H = 0.5

        dict_temp = []
        dict_temp.append(H)
        dict_temp.append(com_name)
        dict_temp.append(currency_name)
        dict_whole.append(dict_temp)

hurst_exp_data = pd.DataFrame(dict_whole, columns = ['h_exponent', 'com_name','commodity/currency'])

"""### TOP 10 trading pairs based on method of mean reversion"""

hurst_exp_data.sort_values(by = ['h_exponent'], ascending=True).head(10)

"""## Combining all four methods"""

dict_whole= []
for com_name in c_name :
    for commodity_name in commdity_name:
        #METHOD 1
        corr_value = df1[com_name].corr(df1[commodity_name])

        #METHOD 2
        _, pvalue, _ = coint(df1[com_name], df1[commodity_name])

        #METHOD 3
        norm_a = np.array(scaled_df1[com_name])
        norm_b = np.array(scaled_df1[commodity_name])
        dist = sum((norm_a - norm_b)**2)
        

        #METHOD 4
        model = LinearRegression().fit(df1[com_name][:, None], df1[commodity_name])
        n = model.coef_
        newSeries = pd.DataFrame()
        newSeries['spread'] = np.log(df1[com_name]) - n * np.log10(df1[commodity_name])
        H = 0.0
        try :
            H, _, _ = compute_Hc(newSeries, kind='price', simplified=True)
        except : 
            H = 0.5


        dict_temp = []        
        dict_temp.append(corr_value)
        dict_temp.append(pvalue)
        dict_temp.append(dist)
        dict_temp.append(H)
        dict_temp.append(com_name)
        dict_temp.append(commodity_name)
        dict_whole.append(dict_temp)


    for currency_name in curr_name:
        dict_temp = []
        if currency_vs_comp(currency_name,com_name):

            #METHOD 1
            corr_value = df2[com_name].corr(df2[currency_name])

            #METHOD 2
            _, pvalue, _ = coint(df2[com_name], df2[currency_name])

            #METHOD 3
            norm_a = np.array(scaled_df2[com_name])
            norm_b = np.array(scaled_df2[currency_name]) 
            dist = sum((norm_a - norm_b)**2) 
            
            #METHOD 4
            model = LinearRegression().fit(df2[com_name][:, None], df2[currency_name])
            n = model.coef_
            newSeries = pd.DataFrame()
            newSeries['spread'] = np.log(df2[com_name]) - n * np.log10(df2[currency_name])
            H = 0.0
            try :
                H, _, _ = compute_Hc(newSeries, kind='price', simplified=True)
            except : 
                H = 0.5

            
            dict_temp.append(corr_value)
            dict_temp.append(pvalue)
            dict_temp.append(dist)
            dict_temp.append(H)

        else :
            dict_temp.append(0)
            dict_temp.append(1)
            dict_temp.append(1e6)
            dict_temp.append(1)

        dict_temp.append(com_name)
        dict_temp.append(currency_name)
        dict_whole.append(dict_temp)

combined_method = pd.DataFrame(dict_whole, columns = ['Corr_val', 'pvalue', 'distance', 'h_exponent', 'com_name','commodity/currency'])

combined_method["currency_match"] = 0;
# usd_dict = {}
# gbp_dict = {}
# sgd_dict = {}
# euro_dict = {}
for index, row in combined_method.iterrows(): 
    temp_name = row["com_name"]
    curr_trade = row["commodity/currency"]
    #temp_value = curr_trade +"INR"
    if (usd_dict[temp_name] or gbp_dict[temp_name] or sgd_dict[temp_name] or euro_dict[temp_name]) and (curr_trade=="EUROINR" or curr_trade=="USDINR" or curr_trade=="SGDINR" or curr_trade=="GBPINR") :
       #row["currency_match"] = 1
       combined_method.at[index,'currency_match'] = 1

combined_method.sort_values(by = ['h_exponent'], ascending=False).head(200)

#combined_method = combined_method[combined_method["Corr_val"]> 0]
combined_method["pvalue"] = combined_method["pvalue"].apply(lambda x : 1/x) 
combined_method["distance"] = combined_method["distance"].apply(lambda x : 1/x)
combined_method["h_exponent"] = combined_method["h_exponent"].apply(lambda x : 1/x)

combined_method.shape

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
combined_method[['Corr_val','pvalue','distance','h_exponent']] = scaler.fit_transform(combined_method[['Corr_val','pvalue','distance','h_exponent']])
#combined_method= pd.DataFrame(new_df1, columns=combined_method.columns,index=combined_method.index)

# industry_comptability = {
#     'SERVICES': [],
#     'CONSUMER GOODS': [],
#     'FINANCIAL SERVICES' :[]
#     'ENERGY': ['CRUDEOIL','NATURALGAS'],
#     'TELECOM': [''],
#     'METALS' : ['NICKEL','ZINC','ALUMINIUM','COPPER', 'LEAD', 'SILVER', 'GOLD'],
#     'IT' : [],
#     'PHARMA' :[],
#     'CONSTRUCTION' :['ALUMINIUM', 'COPPER', 'ZINC'],
#     'AUTOMOBILE' :['GOLD','CRUDEOIL','SILVER','ALUMINIUM', 'LEAD', 'NICKEL', 'NATURALGAS'],
#     'FERTILISERS & PESTICIDES' :['ZINC', 'LEAD'],
#     'CEMENT & CEMENT PRODUCTS' :['ALUMINIUM','LEAD','COPPER', 'ZINC']
#    
# industry_type = {};
# for index, row in df_nifty50stocks.iterrows(): 
#   industry_type[row["Symbol"]] = row["Industry"]
#          }
combined_method["comptability"] = 0;
for index, row in combined_method.iterrows():
  company_stock_name = row["com_name"]
  commi_curr_name = row["commodity/currency"]
  industry = industry_type[company_stock_name]
  #combined_method.at[index,'comptability'] = 1
  if commi_curr_name in industry_comptability[industry] :
    combined_method.at[index,'comptability'] = 1

#combined_method.sample(20)

# Weightage = 35% Corr + 35 % p val + 20% distance + 10% h_exponent
combined_method['weighted_value'] = 0.15* combined_method['Corr_val'] + .75 * combined_method['pvalue'] + .15 * combined_method['distance'] + 0.2 * combined_method['h_exponent'] + .5*combined_method["currency_match"] + 0.05*combined_method["comptability"]
combined_method.sort_values(by = ['weighted_value'], ascending=False).head(10)

submission_df = combined_method.sort_values(by = ['weighted_value'], ascending = False).head(10)
submission_df = submission_df[['com_name', 'commodity/currency']]



submission_df.to_csv('submission.csv', header=False, index=False)
files.download('submission.csv')

